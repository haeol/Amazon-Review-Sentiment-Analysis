{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading in All data for all parsed/cleaned ratings\n",
    "dataTestR1 = pd.read_csv('parsed_data/testing_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTestR1.columns = [\"Review\"]\n",
    "dataTestR1.loc[:,'Rating'] = 1\n",
    "dataTestR2 = pd.read_csv('parsed_data/testing_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTestR2.columns = [\"Review\"]\n",
    "dataTestR2.loc[:,'Rating'] = 2\n",
    "dataTestR3 = pd.read_csv('parsed_data/testing_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTestR3.columns = [\"Review\"]\n",
    "dataTestR3.loc[:,'Rating'] = 3\n",
    "dataTestR4 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR4.columns = [\"Review\"]\n",
    "dataTestR4.loc[:,'Rating'] = 4\n",
    "dataTestR5 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR5.columns = [\"Review\"]\n",
    "dataTestR5.loc[:,'Rating'] = 5\n",
    "\n",
    "dataTrainR1 = pd.read_csv('parsed_data/training_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR1.columns = [\"Review\"]\n",
    "dataTrainR1.loc[:,'Rating'] = 1\n",
    "dataTrainR2 = pd.read_csv('parsed_data/training_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR2.columns = [\"Review\"]\n",
    "dataTrainR2.loc[:,'Rating'] = 2\n",
    "dataTrainR3 = pd.read_csv('parsed_data/training_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR3.columns = [\"Review\"]\n",
    "dataTrainR3.loc[:,'Rating'] = 3\n",
    "dataTrainR4 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR4.columns = [\"Review\"]\n",
    "dataTrainR4.loc[:,'Rating'] = 4\n",
    "dataTrainR5 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR5.columns = [\"Review\"]\n",
    "dataTrainR5.loc[:,'Rating'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avid reader happy book seemed rushed character...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever since action anita blake shifted action s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>historians go dumpster diving cultural effluvi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im big fan michael crichton huge controversy l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>might considered buying ebook look inside show...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  avid reader happy book seemed rushed character...       1\n",
       "1  ever since action anita blake shifted action s...       1\n",
       "2  historians go dumpster diving cultural effluvi...       1\n",
       "3  im big fan michael crichton huge controversy l...       1\n",
       "4  might considered buying ebook look inside show...       1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small Sample Trial Run\n",
    "# training = dataTrainR1 + dataTrainR2 + dataTrainR3 + dataTrainR4 + dataTrainR5\n",
    "# testing = dataTestR1 + dataTestR2 + dataTestR3 + dataTestR4 + dataTestR5\n",
    "framesTrain = [dataTrainR1[:5000],dataTrainR2[:5000],dataTrainR3[:5000],dataTrainR4[:5000],dataTrainR5[:5000]]\n",
    "framesTest = [dataTestR1[:2000], dataTestR2[:2000], dataTestR3[:2000] , dataTestR4[:2000] , dataTestR5[:2000]]\n",
    "training = pd.concat(framesTrain)\n",
    "testing = pd.concat(framesTest)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "framesTrainHalf = [dataTrainR1[:10000],dataTrainR2[:10000],dataTrainR3[:10000],dataTrainR4[:10000],dataTrainR5[:10000]]\n",
    "trainingHalf = pd.concat(framesTrainHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_in_reviews(reviews):\n",
    "    all_words = []\n",
    "    for sentence, rating in zip(training.iloc[:,0], training.iloc[:,1]):\n",
    "        print(sentence, rating)\n",
    "        sentence = str(sentence)\n",
    "        all_words.extend(sentence)\n",
    "    return all_words\n",
    "def get_rating_features(reviewList):\n",
    "    reviewList = nltk.FreqDist(reviewList)\n",
    "    feature = reviewList.keys()\n",
    "    return feature\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' %word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going Through All DataSets\n",
    "for index, row in training.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        training.drop(index, inplace=True)\n",
    "# Going through TrainR1\n",
    "for index, row in dataTrainR1.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR1.drop(index, inplace=True)\n",
    "# Going Through TrainR2\n",
    "for index, row in dataTrainR2.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR2.drop(index, inplace=True)\n",
    "# Going Through TrainR3\n",
    "for index, row in dataTrainR3.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR3.drop(index, inplace=True)\n",
    "# Going Through TrainR4\n",
    "for index, row in dataTrainR4.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR4.drop(index, inplace=True)\n",
    "# Going Through Train R5\n",
    "for index, row in dataTrainR5.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR5.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy book seemed rushed character...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1 of Removing Stop words with data frame\n",
    "stop = stopwords.words('english')\n",
    "# Could add more into Stop words\n",
    "training['Review'] = training['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "training['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avid reader happy book seemed rushed character...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever since action anita blake shifted action s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>historians go dumpster diving cultural effluvi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im big fan michael crichton huge controversy l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>might considered buying ebook look inside show...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  avid reader happy book seemed rushed character...       1\n",
       "1  ever since action anita blake shifted action s...       1\n",
       "2  historians go dumpster diving cultural effluvi...       1\n",
       "3  im big fan michael crichton huge controversy l...       1\n",
       "4  might considered buying ebook look inside show...       1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing for the Others\n",
    "dataTrainR1['Review'] = dataTrainR1['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR2['Review'] = dataTrainR2['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR3['Review'] = dataTrainR3['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR4['Review'] = dataTrainR4['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR5['Review'] = dataTrainR5['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 Tokenize then remove stop words from a sentence input\n",
    "# Tokenize The String Listings\n",
    "# tokenizedList = nltk.word_tokenize(TR)\n",
    "# common = nltk.FreqDist(tokenizedList)\n",
    "# Setting StopWords to those that are in english\n",
    "# stop = stopwords.words('english')\n",
    "# common.most_common(50)\n",
    "# Removing StopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          30628\n",
       "one           15211\n",
       "read          12193\n",
       "like          11978\n",
       "story         10569\n",
       "would          9964\n",
       "good           8466\n",
       "really         7877\n",
       "time           7290\n",
       "get            7242\n",
       "much           6950\n",
       "even           6567\n",
       "great          6321\n",
       "characters     6166\n",
       "first          6129\n",
       "well           5959\n",
       "love           5912\n",
       "books          5671\n",
       "movie          5567\n",
       "dont           5508\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words can or won't do this\n",
    "x = 20\n",
    "freqTotal = pd.Series(' '.join(training['Review']).split()).value_counts()[:x]\n",
    "freqTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Frequent Words for Other Ratings\n",
    "freqR1 = pd.Series(' '.join(dataTrainR1['Review']).split()).value_counts()[:x]\n",
    "freqR2 = pd.Series(' '.join(dataTrainR2['Review']).split()).value_counts()[:x]\n",
    "freqR3 = pd.Series(' '.join(dataTrainR3['Review']).split()).value_counts()[:x]\n",
    "freqR4 = pd.Series(' '.join(dataTrainR4['Review']).split()).value_counts()[:x]\n",
    "freqR5 = pd.Series(' '.join(dataTrainR5['Review']).split()).value_counts()[:x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suffersafter         1\n",
       "nolight              1\n",
       "issuesinstead        1\n",
       "grainits             1\n",
       "veganism             1\n",
       "pyrenees             1\n",
       "cameraim             1\n",
       "hornblower           1\n",
       "goodietwoshoes       1\n",
       "yourselfat           1\n",
       "suburb               1\n",
       "paler                1\n",
       "elsewherethen        1\n",
       "lumbered             1\n",
       "sonypros             1\n",
       "eyeswould            1\n",
       "datedthe             1\n",
       "kinivos              1\n",
       "reasonit             1\n",
       "artifactstreasure    1\n",
       "athomeyogaclass      1\n",
       "levelheaded          1\n",
       "seep                 1\n",
       "muto                 1\n",
       "stocksedleman        1\n",
       "bendis8217           1\n",
       "kushers              1\n",
       "numinous             1\n",
       "ciadini              1\n",
       "shallowlythis        1\n",
       "                    ..\n",
       "elsewhere3take       1\n",
       "turturros            1\n",
       "memoire              1\n",
       "grime                1\n",
       "grandscale           1\n",
       "denominationalism    1\n",
       "seam                 1\n",
       "mpeg4divx            1\n",
       "sessionsmy           1\n",
       "hoodie               1\n",
       "tinkly               1\n",
       "34die                1\n",
       "himgreat             1\n",
       "1000i                1\n",
       "noodles              1\n",
       "puller               1\n",
       "likeabilityleah      1\n",
       "allmany              1\n",
       "prisonerthen         1\n",
       "quandry              1\n",
       "reverie              1\n",
       "10xs                 1\n",
       "eurabia              1\n",
       "carman               1\n",
       "aldridge             1\n",
       "shimada              1\n",
       "featurecooling       1\n",
       "silvia               1\n",
       "journeybug           1\n",
       "quotcountry          1\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rare Words Removal\n",
    "Y = -100\n",
    "rareWordsTotal = pd.Series(' '.join(training['Review']).split()).value_counts()[Y:]\n",
    "rareWordsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do it for the others Probably (get it) Unecessary for comparision\n",
    "rareWordsR1 = pd.Series(' '.join(dataTrainR1['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR2 = pd.Series(' '.join(dataTrainR2['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR3 = pd.Series(' '.join(dataTrainR3['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR4 = pd.Series(' '.join(dataTrainR4['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR5 = pd.Series(' '.join(dataTrainR5['Review']).split()).value_counts()[Y:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of both rare words and common words part 1\n",
    "freqTotal = list(freqTotal.index)\n",
    "rareWordsTota = list(rareWordsTotal.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy seemed rushed history could ...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of both rare and common words part 2\n",
    "training['Review'] = training['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTotal))\n",
    "# training2['Review'].head()\n",
    "training['Review'] = training['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTotal))\n",
    "training['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy seemed rushed history could ...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Half DataSet\n",
    "for index, row in trainingHalf.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingHalf.drop(index, inplace=True)\n",
    "stop = stopwords.words('english')       \n",
    "trainingHalf['Review'] = trainingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "trainingHalf['Review'] = trainingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTotal))\n",
    "# training2['Review'].head()\n",
    "trainingHalf['Review'] = trainingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTotal))\n",
    "trainingHalf['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier Part\n",
    "# Only get the Reviews and tokenize\n",
    "TR = training.to_csv(header=None, index = False)\n",
    "tokenizedList = nltk.word_tokenize(TR)\n",
    "common = nltk.FreqDist(tokenizedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('could', 5340),\n",
       " ('also', 5238),\n",
       " ('way', 5134),\n",
       " ('many', 4834),\n",
       " ('people', 4438),\n",
       " ('know', 4404),\n",
       " ('reading', 4325),\n",
       " ('think', 4295),\n",
       " ('life', 4222),\n",
       " ('little', 4209),\n",
       " ('author', 4193),\n",
       " ('see', 4120),\n",
       " ('two', 4109),\n",
       " ('make', 4027),\n",
       " ('didnt', 4027),\n",
       " ('series', 3866),\n",
       " ('new', 3862),\n",
       " ('never', 3861),\n",
       " ('im', 3834),\n",
       " ('better', 3621),\n",
       " ('back', 3454),\n",
       " ('work', 3433),\n",
       " ('want', 3422),\n",
       " ('character', 3357),\n",
       " ('film', 3349),\n",
       " ('find', 3325),\n",
       " ('still', 3318),\n",
       " ('end', 3214),\n",
       " ('say', 3176),\n",
       " ('found', 3162),\n",
       " ('made', 3086),\n",
       " ('going', 3027),\n",
       " ('something', 3013),\n",
       " ('another', 2967),\n",
       " ('bad', 2841),\n",
       " ('use', 2832),\n",
       " ('go', 2831),\n",
       " ('best', 2812),\n",
       " ('lot', 2771),\n",
       " ('things', 2720),\n",
       " ('plot', 2718),\n",
       " ('got', 2704),\n",
       " ('years', 2658),\n",
       " ('interesting', 2636),\n",
       " ('cant', 2633),\n",
       " ('world', 2604),\n",
       " ('us', 2600),\n",
       " ('novel', 2587),\n",
       " ('doesnt', 2579),\n",
       " ('writing', 2547)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 most common in this total distribution\n",
    "common.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do for the others\n",
    "TR1 = dataTrainR1.to_csv(header=None, index = False)\n",
    "tokenizedListR1 = nltk.word_tokenize(TR1)\n",
    "commonR1 = nltk.FreqDist(tokenizedListR1)\n",
    "\n",
    "TR2 = dataTrainR2.to_csv(header=None, index = False)\n",
    "tokenizedListR2 = nltk.word_tokenize(TR2)\n",
    "commonR2 = nltk.FreqDist(tokenizedListR2)\n",
    "\n",
    "TR3 = dataTrainR3.to_csv(header=None, index = False)\n",
    "tokenizedListR3 = nltk.word_tokenize(TR3)\n",
    "commonR3 = nltk.FreqDist(tokenizedListR3)\n",
    "\n",
    "TR4 = dataTrainR4.to_csv(header=None, index = False)\n",
    "tokenizedListR4 = nltk.word_tokenize(TR4)\n",
    "commonR4 = nltk.FreqDist(tokenizedListR4)\n",
    "\n",
    "TR5 = dataTrainR5.to_csv(header=None, index = False)\n",
    "tokenizedListR5 = nltk.word_tokenize(TR5)\n",
    "commonR5 = nltk.FreqDist(tokenizedListR5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This number is just taken randomly for no apparent reason do for all lists then combine them for getting the most common words\n",
    "WordCnt = 100\n",
    "mostCommonR1 = commonR1.most_common(WordCnt)\n",
    "tempR1 = [x[0] for x in mostCommonR1]\n",
    "mostCommonWordsR1 = set(tempR1)\n",
    "\n",
    "mostCommonR2 = commonR2.most_common(WordCnt)\n",
    "tempR2 = [x[0] for x in mostCommonR2]\n",
    "mostCommonWordsR2 = set(tempR2)\n",
    "\n",
    "mostCommonR3 = commonR3.most_common(WordCnt)\n",
    "tempR3 = [x[0] for x in mostCommonR3]\n",
    "mostCommonWordsR3 = set(tempR3)\n",
    "\n",
    "mostCommonR4 = commonR4.most_common(WordCnt)\n",
    "tempR4 = [x[0] for x in mostCommonR4]\n",
    "mostCommonWordsR4 = set(tempR4)\n",
    "\n",
    "mostCommonR5 = commonR5.most_common(WordCnt)\n",
    "tempR5 = [x[0] for x in mostCommonR5]\n",
    "mostCommonWordsR5 = set(tempR5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining Multiple Bag of Words Sets into One\n",
    "mostCommonWordsTotal = mostCommonWordsR1\n",
    "mostCommonWordsTotal.update(mostCommonWordsR2)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR3)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR4)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR5)\n",
    "len(mostCommonWordsTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half DataSet\n",
    "TRHalf = trainingHalf.to_csv(header=None, index = False)\n",
    "tokenizedListHalf = nltk.word_tokenize(TR)\n",
    "commonHalf = nltk.FreqDist(tokenizedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for small\n",
    "ResultTupOD = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in training.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                1 : 4      =     26.4 : 1.0\n",
      "                  highly = True                5 : 3      =      4.7 : 1.0\n",
      "                   money = True                1 : 4      =      4.5 : 1.0\n",
      "               wonderful = True                5 : 1      =      4.0 : 1.0\n",
      "                   loved = True                5 : 1      =      3.9 : 1.0\n",
      "                 enjoyed = True                3 : 1      =      3.8 : 1.0\n",
      "                     bad = True                1 : 4      =      3.7 : 1.0\n",
      "                   liked = True                3 : 1      =      3.6 : 1.0\n",
      "                   didnt = True                2 : 4      =      3.3 : 1.0\n",
      "                   wasnt = True                2 : 4      =      3.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier SMALL\n",
    "classifierSmall = nltk.NaiveBayesClassifier.train(ResultTupOD)\n",
    "classifierSmall.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what a shame i was really looking forward to a period drama worthy of source material instead i got artsyfartsy rubbish the actors do fine but they seem out of place in this strange conglomeration of stage decorations which only seems to be there for one of two reasons a to show how ohsooriginal the filmmakers are not or b because they didnt want to spend a ton of money on actual realistic scenes of the glorious gorgeous 19th century russia and its high society even though the movie obviously thinks itself ohsooriginal it really isnt how many times have you seen a variation of time stopping as two people are falling in love showing it from a different angle doesnt make it original last but not least anyone care to explain why kremlin which is in moscow is a recurring background for scenes that are supposed to be happening in st petersburg looks like they saved some money on consultants as well so is it epic yes an epic disappointment :  2\n",
      "True:  2  small:  2  half:  2\n"
     ]
    }
   ],
   "source": [
    "# Testing For Small and half\n",
    "# This will randomly select an object from dataset and compare it \n",
    "\n",
    "row = testing.sample(1)\n",
    "test_sentence = row.iloc[0]['Review']\n",
    "rating = row.iloc[0]['Rating']\n",
    "print(test_sentence, \": \", rating)\n",
    "\n",
    "test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in mostCommonWordsTotal}\n",
    "resultSmall = classifierSmall.classify(test_sent_features)\n",
    "resultHalf = classifierHalf.classify(test_sent_features)\n",
    "\n",
    "print(\"True: \" , rating , \" small: \", resultSmall, \" half: \", resultHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NaiveBayesClassifier' object has no attribute 'pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-f31e44774cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TESTING PICKLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifierHalf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclassifierPickleHalf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NaiveBayesClassifier' object has no attribute 'pkl'"
     ]
    }
   ],
   "source": [
    "# TESTING PICKLE\n",
    "with open(classifierHalf.pkl, 'rb') as file:  \n",
    "    classifierPickleHalf = pickle.load(file)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  This is for Half\n",
    "ResultTupODHalf = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in trainingHalf.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                1 : 4      =     27.3 : 1.0\n",
      "                   money = True                1 : 4      =      4.5 : 1.0\n",
      "                  highly = True                5 : 2      =      4.4 : 1.0\n",
      "               wonderful = True                5 : 1      =      4.2 : 1.0\n",
      "                 enjoyed = True                3 : 1      =      3.9 : 1.0\n",
      "                   loved = True                5 : 1      =      3.8 : 1.0\n",
      "                   liked = True                3 : 1      =      3.7 : 1.0\n",
      "                     bad = True                1 : 4      =      3.4 : 1.0\n",
      "                    wait = True                5 : 3      =      3.3 : 1.0\n",
      "                     bit = True                3 : 1      =      3.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier Part\n",
    "classifierHalf = nltk.NaiveBayesClassifier.train(ResultTupODHalf)\n",
    "classifierHalf.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-29 10:53:44.534583\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Just to See When Finished\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"classifierSmall.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierSmall, file)\n",
    "\n",
    "# Load from file\n",
    "# with open(pkl_filename, 'rb') as file:  \n",
    "#     pickle_model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-aaa24a87b21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is the best band I've ever heard!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_sent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mResultTupODHalf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mclassifierHalf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-aaa24a87b21d>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is the best band I've ever heard!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_sent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mResultTupODHalf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mclassifierHalf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# print(tryPredicting(\"\"))\n",
    "# def tryPredicting(testSentence):\n",
    "#     test_sent_features = {word.lower(): (word in nltk.word_tokenize(testSentence.lower())) for word in ResultTupOD}\n",
    "#     return classifierSmall.classify(test_sent_features)\n",
    "\n",
    "# testingSample = \"\"\n",
    "# outcomeRating = 1\n",
    "# print(tryPredicting(test_sentence))\n",
    "\n",
    "test_sentence = \"This is the best band I've ever heard!\"\n",
    "test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in ResultTupODHalf}\n",
    "classifierHalf.classify(test_sent_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official Continuation of full datasize Method 2 (was working fine but took way too long)\n",
    "# training.head()\n",
    "all_wordsBag = set(word.lower() for passage in training['Review'] for word in nltk.word_tokenize(passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520643"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_wordsBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResultTupOD = [({word: (word in nltk.word_tokenize(row['Review'])) for word in all_wordsBag}, row['Rating']) for index,row in training.iterrows()]\n",
    "# classifierBae = nltk.NaiveBayesClassifier.train(ResultTupOD)\n",
    "# classifierBae.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read many books general topic thought onewas w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loved 34the girl dragon tattoo34 series though...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kelsey grammar cant take seriously hard ass st...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty disappointed book expecting bowled lang...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one shahs books popularized sufi path america ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love brides kindred series little disappointed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prince jorg mission revenge revenge fallen mot...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone familiar line merchant venice question ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>til death us part compilation 15 short stories...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis seemed promising clear mind work spen...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  read many books general topic thought onewas w...       2\n",
       "1  loved 34the girl dragon tattoo34 series though...       2\n",
       "2  kelsey grammar cant take seriously hard ass st...       2\n",
       "3  pretty disappointed book expecting bowled lang...       2\n",
       "4  one shahs books popularized sufi path america ...       2\n",
       "0  love brides kindred series little disappointed...       3\n",
       "1  prince jorg mission revenge revenge fallen mot...       3\n",
       "2  anyone familiar line merchant venice question ...       3\n",
       "3  til death us part compilation 15 short stories...       3\n",
       "4  synopsis seemed promising clear mind work spen...       3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing/Trial An Error Part\n",
    "# tempTrainR1 = dataTrainR1[:2].to_csv(header=None, index = False)\n",
    "tempTrainR1 = dataTrainR1[:2]\n",
    "tempTrainR2 = dataTrainR2[:5]\n",
    "tempTrainR3 = dataTrainR3[:5]\n",
    "framesTempTrain = [tempTrainR2, tempTrainR3]\n",
    "\n",
    "tempTrainTot = pd.concat(framesTempTrain)\n",
    "# tokenListTrainR1 = nltk.word_tokenize(tempTrainR1)\n",
    "# tokenListTrainR1\n",
    "# tempTrainTot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempTrainR1['Review'] = tempTrainR1.apply(lambda row: nltk.word_tokenize(row['Review']), axis=1) Consider Doing Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'15',\n",
       " '34',\n",
       " '34the',\n",
       " '82038203me',\n",
       " '911',\n",
       " 'abruptly',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'additionthe',\n",
       " 'admit',\n",
       " 'adults',\n",
       " 'advocate',\n",
       " 'afraid',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'ali',\n",
       " 'allies',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'alongside',\n",
       " 'also',\n",
       " 'alternates',\n",
       " 'although',\n",
       " 'america',\n",
       " 'anarchistthis',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'apart',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appeared',\n",
       " 'around',\n",
       " 'ass',\n",
       " 'assist',\n",
       " 'attempt',\n",
       " 'audience',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'autobiographical',\n",
       " 'background',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'beg',\n",
       " 'ben',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'blockbuster',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bordering',\n",
       " 'bored',\n",
       " 'bowled',\n",
       " 'box',\n",
       " 'braggingthere',\n",
       " 'brides',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'building',\n",
       " 'bumming',\n",
       " 'buried',\n",
       " 'c',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'certainly',\n",
       " 'challenging',\n",
       " 'champions',\n",
       " 'change',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'charmed',\n",
       " 'chemistry',\n",
       " 'chicken',\n",
       " 'childish',\n",
       " 'chore',\n",
       " 'citys',\n",
       " 'claimed',\n",
       " 'class',\n",
       " 'classprofessional',\n",
       " 'clear',\n",
       " 'cliches',\n",
       " 'closer',\n",
       " 'coldhearted',\n",
       " 'comedy',\n",
       " 'comical',\n",
       " 'commitmentbut',\n",
       " 'compilation',\n",
       " 'complains',\n",
       " 'concur',\n",
       " 'conditions',\n",
       " 'confused',\n",
       " 'considered',\n",
       " 'constantly',\n",
       " 'continue',\n",
       " 'contrived',\n",
       " 'convinced',\n",
       " 'corrosive',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'counter',\n",
       " 'cover',\n",
       " 'cringethis',\n",
       " 'culture',\n",
       " 'current',\n",
       " 'dawson',\n",
       " 'death',\n",
       " 'debut',\n",
       " 'dedicated',\n",
       " 'defiantly',\n",
       " 'degradation',\n",
       " 'depression',\n",
       " 'describe',\n",
       " 'deserving',\n",
       " 'desperately',\n",
       " 'despite',\n",
       " 'destruction',\n",
       " 'deter',\n",
       " 'didnt',\n",
       " 'difficult',\n",
       " 'disabled',\n",
       " 'disappointed',\n",
       " 'dissecting',\n",
       " 'disturbing',\n",
       " 'diverse',\n",
       " 'divorce',\n",
       " 'doctrine',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'dont',\n",
       " 'doors',\n",
       " 'draft',\n",
       " 'dragon',\n",
       " 'drama',\n",
       " 'dramatic',\n",
       " 'driven',\n",
       " 'editing',\n",
       " 'effort',\n",
       " 'either',\n",
       " 'elicit',\n",
       " 'elseultimately',\n",
       " 'emergency',\n",
       " 'emily',\n",
       " 'emotional',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enough',\n",
       " 'entails',\n",
       " 'entirely',\n",
       " 'erroneously',\n",
       " 'especially',\n",
       " 'essence',\n",
       " 'evening',\n",
       " 'evenings',\n",
       " 'events',\n",
       " 'ever',\n",
       " 'everything',\n",
       " 'exist',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'expecting',\n",
       " 'experience',\n",
       " 'eyes',\n",
       " 'fallen',\n",
       " 'falsely',\n",
       " 'familiar',\n",
       " 'fanny',\n",
       " 'fantasized',\n",
       " 'far',\n",
       " 'favorite',\n",
       " 'fed',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'film',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'flesh',\n",
       " 'follow',\n",
       " 'followed',\n",
       " 'food',\n",
       " 'forced',\n",
       " 'formulated',\n",
       " 'forth',\n",
       " 'forward',\n",
       " 'four',\n",
       " 'fourteen',\n",
       " 'frager',\n",
       " 'franklin',\n",
       " 'freedom',\n",
       " 'front',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'genre',\n",
       " 'get',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'good',\n",
       " 'goodreads',\n",
       " 'grammar',\n",
       " 'great',\n",
       " 'group',\n",
       " 'guess',\n",
       " 'guessed',\n",
       " 'guts',\n",
       " 'guy',\n",
       " 'hand',\n",
       " 'hang',\n",
       " 'haphazardly',\n",
       " 'happy',\n",
       " 'happyness',\n",
       " 'hard',\n",
       " 'harrelson',\n",
       " 'hating',\n",
       " 'heart',\n",
       " 'heartbroken',\n",
       " 'hearts',\n",
       " 'heroes',\n",
       " 'himselfthe',\n",
       " 'hippie',\n",
       " 'hm',\n",
       " 'holiday',\n",
       " 'honest',\n",
       " 'however',\n",
       " 'idea',\n",
       " 'ideal',\n",
       " 'identify',\n",
       " 'idries',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'imagine',\n",
       " 'importance',\n",
       " 'inconsistencies',\n",
       " 'individual',\n",
       " 'inevitable',\n",
       " 'info',\n",
       " 'ingredients',\n",
       " 'inspire',\n",
       " 'interesting',\n",
       " 'introduction',\n",
       " 'irony',\n",
       " 'irs',\n",
       " 'jerk',\n",
       " 'jk',\n",
       " 'jointshilarious',\n",
       " 'jorg',\n",
       " 'journey',\n",
       " 'judgment',\n",
       " 'kate',\n",
       " 'kates',\n",
       " 'keeping',\n",
       " 'keeps',\n",
       " 'kelsey',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kindred',\n",
       " 'knows',\n",
       " 'lacks',\n",
       " 'language',\n",
       " 'lapse',\n",
       " 'lawyer',\n",
       " 'leaderthe',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'letting',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'line',\n",
       " 'literature',\n",
       " 'little',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovein',\n",
       " 'lucienewbooksonmyselvesblogspotfr',\n",
       " 'made',\n",
       " 'magic',\n",
       " 'main',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'make',\n",
       " 'many',\n",
       " 'mark',\n",
       " 'marriage',\n",
       " 'master',\n",
       " 'mean',\n",
       " 'mebut',\n",
       " 'meet',\n",
       " 'merchant',\n",
       " 'middle',\n",
       " 'miller',\n",
       " 'mind',\n",
       " 'minebut',\n",
       " 'minimum',\n",
       " 'miserable',\n",
       " 'mission',\n",
       " 'momentary',\n",
       " 'moments',\n",
       " 'money',\n",
       " 'morning34',\n",
       " 'mostly',\n",
       " 'mother',\n",
       " 'motivations',\n",
       " 'mouththe',\n",
       " 'movie',\n",
       " 'mr',\n",
       " 'much',\n",
       " 'must',\n",
       " 'muzaffer',\n",
       " 'mystery',\n",
       " 'naturally',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'newage',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nihilism',\n",
       " 'nonplot',\n",
       " 'not',\n",
       " 'novel',\n",
       " 'number',\n",
       " 'numerous',\n",
       " 'office',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'one',\n",
       " 'onedimensional',\n",
       " 'onewas',\n",
       " 'operate',\n",
       " 'opinion',\n",
       " 'order',\n",
       " 'orirshad',\n",
       " 'otr',\n",
       " 'outer',\n",
       " 'ozak',\n",
       " 'palpable',\n",
       " 'paris',\n",
       " 'part',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'passion',\n",
       " 'past',\n",
       " 'path',\n",
       " 'pay',\n",
       " 'payday',\n",
       " 'pedestrian',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'personalitiesthe',\n",
       " 'philosophical',\n",
       " 'philosophize',\n",
       " 'philosophy',\n",
       " 'pick',\n",
       " 'place',\n",
       " 'placed',\n",
       " 'plain',\n",
       " 'play',\n",
       " 'played',\n",
       " 'plays',\n",
       " 'plot',\n",
       " 'point',\n",
       " 'polished',\n",
       " 'popularized',\n",
       " 'posa',\n",
       " 'position',\n",
       " 'potty',\n",
       " 'pound',\n",
       " 'predict',\n",
       " 'present',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'prince',\n",
       " 'princely',\n",
       " 'promising',\n",
       " 'propelling',\n",
       " 'prove',\n",
       " 'pursuit',\n",
       " 'pushes',\n",
       " 'quest',\n",
       " 'question',\n",
       " 'rather',\n",
       " 'rays',\n",
       " 'reactions',\n",
       " 'read',\n",
       " 'readersso',\n",
       " 'reading',\n",
       " 'realism',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'recommend',\n",
       " 'redemptive',\n",
       " 'refined',\n",
       " 'regardless',\n",
       " 'regret',\n",
       " 'relationships',\n",
       " 'report',\n",
       " 'represented',\n",
       " 'requested',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'resulting',\n",
       " 'revenge',\n",
       " 'reviews',\n",
       " 'revolves',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'role',\n",
       " 'romance',\n",
       " 'romantic',\n",
       " 'rosario',\n",
       " 'rough',\n",
       " 'roving',\n",
       " 'rummage',\n",
       " 'running',\n",
       " 'sacrifice',\n",
       " 'sainthood',\n",
       " 'sale',\n",
       " 'salvation',\n",
       " 'satisfactory',\n",
       " 'scandinavian',\n",
       " 'scholarship',\n",
       " 'score',\n",
       " 'screen',\n",
       " 'scripting',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'seemed',\n",
       " 'seems',\n",
       " 'sees',\n",
       " 'self',\n",
       " 'selfabsorbedwill',\n",
       " 'selfproclaimed',\n",
       " 'sell',\n",
       " 'sellsword',\n",
       " 'sensual',\n",
       " 'sequel',\n",
       " 'serie',\n",
       " 'series',\n",
       " 'seriously',\n",
       " 'set',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'sex',\n",
       " 'sexologist',\n",
       " 'sexual',\n",
       " 'shah',\n",
       " 'shahs',\n",
       " 'short',\n",
       " 'sighing',\n",
       " 'since',\n",
       " 'situations',\n",
       " 'six',\n",
       " 'slade',\n",
       " 'sleep',\n",
       " 'slightly',\n",
       " 'smile',\n",
       " 'smileadmittedly',\n",
       " 'smith',\n",
       " 'smiths',\n",
       " 'smok',\n",
       " 'snippets',\n",
       " 'society',\n",
       " 'sojourn',\n",
       " 'something',\n",
       " 'sorry',\n",
       " 'soul',\n",
       " 'souls',\n",
       " 'speaking',\n",
       " 'speech',\n",
       " 'spend',\n",
       " 'spiritual',\n",
       " 'stability',\n",
       " 'start',\n",
       " 'started',\n",
       " 'staying',\n",
       " 'stop',\n",
       " 'stories',\n",
       " 'story',\n",
       " 'stripped',\n",
       " 'strong',\n",
       " 'struggle',\n",
       " 'successful',\n",
       " 'suffuses',\n",
       " 'sufi',\n",
       " 'sufism',\n",
       " 'suicide',\n",
       " 'summertime',\n",
       " 'supernatural',\n",
       " 'support',\n",
       " 'supporting',\n",
       " 'surprised',\n",
       " 'swords',\n",
       " 'symbol',\n",
       " 'synopsis',\n",
       " 'system',\n",
       " 'take',\n",
       " 'tattoo34',\n",
       " 'taut',\n",
       " 'tearing',\n",
       " 'template',\n",
       " 'ten',\n",
       " 'tends',\n",
       " 'theni',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thirty',\n",
       " 'thomas',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'threw',\n",
       " 'tidbits',\n",
       " 'til',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'titles',\n",
       " 'toc',\n",
       " 'toggles',\n",
       " 'topic',\n",
       " 'touch',\n",
       " 'towards',\n",
       " 'transplant',\n",
       " 'traumatic',\n",
       " 'travel',\n",
       " 'true',\n",
       " 'try',\n",
       " 'turns',\n",
       " 'two',\n",
       " 'ugly',\n",
       " 'underlying',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unrealistic',\n",
       " 'unveiled',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'variably',\n",
       " 'venice',\n",
       " 'viewed',\n",
       " 'viewer',\n",
       " 'viewing',\n",
       " 'violent',\n",
       " 'virgin',\n",
       " 'visceral',\n",
       " 'vitriolic',\n",
       " 'walked',\n",
       " 'want',\n",
       " 'wasnt',\n",
       " 'watered',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'well',\n",
       " 'westernized',\n",
       " 'white',\n",
       " 'whites',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'woody',\n",
       " 'word',\n",
       " 'work',\n",
       " 'works',\n",
       " 'worth',\n",
       " 'worthy',\n",
       " 'would',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wry',\n",
       " 'years',\n",
       " 'young'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method2\n",
    "# tup = tempTrainR2['Review'][0]\n",
    "# type(tup)\n",
    "all_wordsFinal = set(word.lower() for passage in tempTrainTot['Review'] for word in nltk.word_tokenize(passage))\n",
    "# all_wordsR3 = set(word.lower() for passage in tempTrainR3['Review'] for word in nltk.word_tokenize(passage))\n",
    "# all_wordsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  little = False               2 : 3      =      3.0 : 1.0\n",
      "                  little = True                3 : 2      =      3.0 : 1.0\n",
      "                 reading = True                3 : 2      =      2.3 : 1.0\n",
      "                   story = False               2 : 3      =      2.3 : 1.0\n",
      "                  really = True                3 : 2      =      2.3 : 1.0\n",
      "                  really = False               2 : 3      =      1.8 : 1.0\n",
      "                 reading = False               2 : 3      =      1.8 : 1.0\n",
      "                   story = True                3 : 2      =      1.8 : 1.0\n",
      "                    read = True                2 : 3      =      1.7 : 1.0\n",
      "            disappointed = True                3 : 2      =      1.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# t = [({word: (word in nltk.word_tokenize(x[0])) for word in all_wordsSample}, x[1]) for x in trainSample]\n",
    "# trialR2\n",
    "#       {word: (word in nltk.word_tokenize(x[0])) for word in all_wordsSample}\n",
    "#     for word in all_wordsSample\n",
    "# Make dictionary \n",
    "# word : word in nltk.word_tokenize(x[0]))\n",
    "\n",
    "# This will give you a tuple of dictionary features, basically saying if this is present or not and what category it is in\n",
    "temp = [({word: (word in nltk.word_tokenize(row['Review'])) for word in all_wordsR2}, row['Rating']) for index,row in tempTrainTot.iterrows()]\n",
    "\n",
    "# x[0] should be the review\n",
    "# x[1] should be the rating\n",
    "# for index,row in tempTrainR2.iterrows():\n",
    "#     print(row['Review'], \" : \", row['Rating'])\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(temp)\n",
    "classifier2.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tryPredicting(testSentence):\n",
    "    test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in all_wordsFinal}\n",
    "    return classifier2.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# print(tryPredicting(\"\"))\n",
    "def tryPredicting(testSentence):\n",
    "    test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in all_wordsFinal}\n",
    "    return classifier2.classify(test_sent_features)\n",
    "\n",
    "testingSample = \"\"\n",
    "outcomeRating = 1\n",
    "for index,row in dataTestR2.iterrows():\n",
    "    testingSample = row['Review']\n",
    "    outcomeRating = row['Rating']\n",
    "    break\n",
    "print(tryPredicting(testingSample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSample = [('I love this sandwich.', 'pos'),\n",
    "('This is an amazing place!', 'pos'),\n",
    "('I feel very good about these beers.', 'pos'),\n",
    "('This is my best work.', 'pos'),\n",
    "(\"What an awesome view\", 'pos'),\n",
    "('I do not like this restaurant', 'neg'),\n",
    "('I am tired of this stuff.', 'neg'),\n",
    "(\"I can't deal with this\", 'neg'),\n",
    "('He is my sworn enemy!', 'neg'),\n",
    "('My boss is horrible.', 'neg')]\n",
    "all_wordsSample = set(word.lower() for passage in trainSample for word in nltk.word_tokenize(passage[0]))\n",
    "type(all_wordsSample)\n",
    "# all_wordsSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in trainSample:\n",
    "#     print(x)\n",
    "#     for word in nltk.word_tokenize(x[0]):\n",
    "#         print(type(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': True,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': True,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': True,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': True,\n",
       "   'an': True,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': True,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': True,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': True,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': True,\n",
       "   'good': True,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': True,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': True,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': True,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': True,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': True},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': True,\n",
       "   'awesome': True,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': True,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': True,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': True,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': True,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': True,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': True,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': True,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': True,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': True,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': False,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': True,\n",
       "   'deal': True,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": True,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': True,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': True,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': True,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': True,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': True,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': True,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': True,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [({word: (word in nltk.word_tokenize(x[0])) for word in all_wordsSample}, x[1]) for x in trainSample]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    this = True              neg : pos    =      2.3 : 1.0\n",
      "                    this = False             pos : neg    =      1.8 : 1.0\n",
      "                      an = False             neg : pos    =      1.6 : 1.0\n",
      "                       . = True              pos : neg    =      1.4 : 1.0\n",
      "                       . = False             neg : pos    =      1.4 : 1.0\n",
      "              restaurant = False             pos : neg    =      1.2 : 1.0\n",
      "                     not = False             pos : neg    =      1.2 : 1.0\n",
      "                   these = False             neg : pos    =      1.2 : 1.0\n",
      "                   place = False             neg : pos    =      1.2 : 1.0\n",
      "                      am = False             pos : neg    =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct The Classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = \"This is the best band I've ever heard!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in all_wordsSample}\n",
    "classifier.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
