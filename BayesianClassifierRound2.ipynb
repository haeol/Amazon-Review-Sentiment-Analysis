{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import pickle\n",
    "# from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Models With PICKLE\n",
    "with open(\"classifierHalf.pkl\", 'rb') as file:  \n",
    "    classifierPickleHalf = pickle.load(file)\n",
    "    \n",
    "with open(\"classifierNormal.pkl\", 'rb') as file:  \n",
    "    classifierNormal = pickle.load(file) \n",
    "\n",
    "with open(\"classifierSmall.pkl\", 'rb') as file:  \n",
    "    classifierSmall = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading in All data for all parsed/cleaned ratings\n",
    "dataTestR1 = pd.read_csv('parsed_data/testing_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTestR1.columns = [\"Review\"]\n",
    "dataTestR1.loc[:,'Rating'] = 1\n",
    "dataTestR2 = pd.read_csv('parsed_data/testing_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTestR2.columns = [\"Review\"]\n",
    "dataTestR2.loc[:,'Rating'] = 2\n",
    "dataTestR3 = pd.read_csv('parsed_data/testing_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTestR3.columns = [\"Review\"]\n",
    "dataTestR3.loc[:,'Rating'] = 3\n",
    "dataTestR4 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR4.columns = [\"Review\"]\n",
    "dataTestR4.loc[:,'Rating'] = 4\n",
    "dataTestR5 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR5.columns = [\"Review\"]\n",
    "dataTestR5.loc[:,'Rating'] = 5\n",
    "\n",
    "dataTrainR1 = pd.read_csv('parsed_data/training_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR1.columns = [\"Review\"]\n",
    "dataTrainR1.loc[:,'Rating'] = 1\n",
    "dataTrainR2 = pd.read_csv('parsed_data/training_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR2.columns = [\"Review\"]\n",
    "dataTrainR2.loc[:,'Rating'] = 2\n",
    "dataTrainR3 = pd.read_csv('parsed_data/training_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR3.columns = [\"Review\"]\n",
    "dataTrainR3.loc[:,'Rating'] = 3\n",
    "dataTrainR4 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR4.columns = [\"Review\"]\n",
    "dataTrainR4.loc[:,'Rating'] = 4\n",
    "dataTrainR5 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR5.columns = [\"Review\"]\n",
    "dataTrainR5.loc[:,'Rating'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avid reader happy book seemed rushed character...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever since action anita blake shifted action s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>historians go dumpster diving cultural effluvi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im big fan michael crichton huge controversy l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>might considered buying ebook look inside show...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  avid reader happy book seemed rushed character...       1\n",
       "1  ever since action anita blake shifted action s...       1\n",
       "2  historians go dumpster diving cultural effluvi...       1\n",
       "3  im big fan michael crichton huge controversy l...       1\n",
       "4  might considered buying ebook look inside show...       1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AggregatingMore into each test Full 80 to 20% for random values\n",
    "framesTrainFull = [dataTrainR1,dataTrainR2,dataTrainR3,dataTrainR4,dataTrainR5]\n",
    "framesTestFull = [dataTestR1, dataTestR2, dataTestR3 , dataTestR4 , dataTestR5]\n",
    "trainingFull = pd.concat(framesTrainFull)\n",
    "testingFull = pd.concat(framesTestFull)\n",
    "\n",
    "# test Half\n",
    "framesTrainHalf = [dataTrainR1.sample(n=40000),dataTrainR2.sample(n=40000),dataTrainR3.sample(n=40000),dataTrainR4.sample(40000),dataTrainR5.sample(n=40000)]\n",
    "framesTestHalf = [dataTestR1.sample(10000), dataTestR2.sample(10000), dataTestR3.sample(10000) , dataTestR4.sample(10000) , dataTestR5.sample(10000)]\n",
    "trainingHalf = pd.concat(framesTrainHalf)\n",
    "testingHalf = pd.concat(framesTestHalf)\n",
    "# Test Small\n",
    "framesTrainSmall = [dataTrainR1.sample(10000),dataTrainR2.sample(10000),dataTrainR3.sample(10000),dataTrainR4.sample(10000),dataTrainR5.sample(10000)]\n",
    "framesTestSmall = [dataTestR1.sample(2500), dataTestR2.sample(2500), dataTestR3.sample(2500) , dataTestR4.sample(2500) , dataTestR5.sample(2500)]\n",
    "trainingSmall = pd.concat(framesTrainSmall)\n",
    "testingSmall = pd.concat(framesTestSmall)\n",
    "trainingFull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>bad bad really really bad movie good mention m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8828</th>\n",
       "      <td>author offers briefly fairytale version orthod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25841</th>\n",
       "      <td>book ought titled mark beast sense talks god d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42906</th>\n",
       "      <td>uncomfortable head buds ever invented feels li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18768</th>\n",
       "      <td>bought primarily ability play music sd card us...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating\n",
       "2233   bad bad really really bad movie good mention m...       1\n",
       "8828   author offers briefly fairytale version orthod...       1\n",
       "25841  book ought titled mark beast sense talks god d...       1\n",
       "42906  uncomfortable head buds ever invented feels li...       1\n",
       "18768  bought primarily ability play music sd card us...       1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingHalf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# framesTrainHalf = [dataTrainR1[:10000],dataTrainR2[:10000],dataTrainR3[:10000],dataTrainR4[:10000],dataTrainR5[:10000]]\n",
    "# trainingHalf = pd.concat(framesTrainHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_in_reviews(reviews):\n",
    "    all_words = []\n",
    "    for sentence, rating in zip(training.iloc[:,0], training.iloc[:,1]):\n",
    "        print(sentence, rating)\n",
    "        sentence = str(sentence)\n",
    "        all_words.extend(sentence)\n",
    "    return all_words\n",
    "def get_rating_features(reviewList):\n",
    "    reviewList = nltk.FreqDist(reviewList)\n",
    "    feature = reviewList.keys()\n",
    "    return feature\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' %word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going Through All DataSets\n",
    "for index, row in trainingFull.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingFull.drop(index, inplace=True)\n",
    "for index, row in trainingSmall.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingSmall.drop(index, inplace=True)\n",
    "for index, row in trainingHalf.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingHalf.drop(index, inplace=True)\n",
    "        \n",
    "# Going through TrainR1\n",
    "for index, row in dataTrainR1.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR1.drop(index, inplace=True)\n",
    "# Going Through TrainR2\n",
    "for index, row in dataTrainR2.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR2.drop(index, inplace=True)\n",
    "# Going Through TrainR3\n",
    "for index, row in dataTrainR3.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR3.drop(index, inplace=True)\n",
    "# Going Through TrainR4\n",
    "for index, row in dataTrainR4.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR4.drop(index, inplace=True)\n",
    "# Going Through Train R5\n",
    "for index, row in dataTrainR5.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR5.drop(index, inplace=True)\n",
    "        \n",
    "# Going Through TestingSet\n",
    "for index, row in testingFull.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        testingFull.drop(index, inplace=True)\n",
    "for index, row in testingSmall.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        testingSmall.drop(index, inplace=True)\n",
    "for index, row in testingHalf.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        testingHalf.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy book seemed rushed character...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stopwords, rare words from datasets\n",
    "stop = stopwords.words('english')\n",
    "trainingFull['Review'] = trainingFull['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "testingFull['Review'] = testingFull['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "trainingFull['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as above but just for the rest\n",
    "trainingHalf['Review'] = trainingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "testingHalf['Review'] = testingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "trainingSmall['Review'] = trainingSmall['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "testingSmall['Review'] = testingSmall['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing for the Others\n",
    "dataTrainR1['Review'] = dataTrainR1['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR2['Review'] = dataTrainR2['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR3['Review'] = dataTrainR3['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR4['Review'] = dataTrainR4['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR5['Review'] = dataTrainR5['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          59889\n",
       "one           30313\n",
       "like          24115\n",
       "read          23895\n",
       "story         21729\n",
       "would         20067\n",
       "good          16932\n",
       "really        15617\n",
       "get           14598\n",
       "time          14346\n",
       "much          14163\n",
       "even          13212\n",
       "characters    12476\n",
       "first         12315\n",
       "great         12241\n",
       "love          11801\n",
       "well          11750\n",
       "books         11591\n",
       "movie         10999\n",
       "dont          10996\n",
       "dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words\n",
    "x = 20\n",
    "# For Small\n",
    "freqTotalSmall = pd.Series(' '.join(trainingSmall['Review']).split()).value_counts()[:x]\n",
    "freqTestingTotalSmall = pd.Series(' '.join(testingSmall['Review']).split()).value_counts()[:x]\n",
    "freqTotalSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          241267\n",
       "one           121884\n",
       "like           96961\n",
       "read           95207\n",
       "story          87497\n",
       "would          79705\n",
       "good           68508\n",
       "really         62342\n",
       "get            58181\n",
       "time           57392\n",
       "much           56987\n",
       "even           52540\n",
       "first          50464\n",
       "characters     49724\n",
       "great          49680\n",
       "love           47725\n",
       "well           47157\n",
       "books          46140\n",
       "dont           44313\n",
       "movie          44018\n",
       "dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words\n",
    "# For Half\n",
    "freqTotalHalf = pd.Series(' '.join(trainingHalf['Review']).split()).value_counts()[:x]\n",
    "freqTestingTotalHalf = pd.Series(' '.join(testingHalf['Review']).split()).value_counts()[:x]\n",
    "freqTotalHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          302098\n",
       "one           152305\n",
       "like          121311\n",
       "read          119507\n",
       "story         109497\n",
       "would          99940\n",
       "good           85693\n",
       "really         77983\n",
       "get            72910\n",
       "time           71818\n",
       "much           71251\n",
       "even           66033\n",
       "first          63016\n",
       "characters     62089\n",
       "great          61991\n",
       "love           59676\n",
       "well           59244\n",
       "books          57676\n",
       "dont           55605\n",
       "movie          55050\n",
       "dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words\n",
    "# For Full\n",
    "freqTotalFull = pd.Series(' '.join(trainingFull['Review']).split()).value_counts()[:x]\n",
    "freqTestingTotalFull = pd.Series(' '.join(testingFull['Review']).split()).value_counts()[:x]\n",
    "freqTotalFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suffersafter',\n",
       " 'nolight',\n",
       " 'issuesinstead',\n",
       " 'grainits',\n",
       " 'veganism',\n",
       " 'pyrenees',\n",
       " 'cameraim',\n",
       " 'hornblower',\n",
       " 'goodietwoshoes',\n",
       " 'yourselfat',\n",
       " 'suburb',\n",
       " 'paler',\n",
       " 'elsewherethen',\n",
       " 'lumbered',\n",
       " 'sonypros',\n",
       " 'eyeswould',\n",
       " 'datedthe',\n",
       " 'kinivos',\n",
       " 'reasonit',\n",
       " 'artifactstreasure',\n",
       " 'athomeyogaclass',\n",
       " 'levelheaded',\n",
       " 'seep',\n",
       " 'muto',\n",
       " 'stocksedleman',\n",
       " 'bendis8217',\n",
       " 'kushers',\n",
       " 'numinous',\n",
       " 'ciadini',\n",
       " 'shallowlythis',\n",
       " 'f15s',\n",
       " 'connectionbug',\n",
       " 'oneup',\n",
       " 'yakking',\n",
       " 'followhorror',\n",
       " 'sandiskthey',\n",
       " 'finesoeither',\n",
       " 'war1',\n",
       " 'notdoes',\n",
       " 'arentbrucebatman',\n",
       " 'timewaster',\n",
       " 'mediumsize',\n",
       " 'intruiging',\n",
       " 'benders',\n",
       " '10mbso',\n",
       " 'goinglike',\n",
       " 'duns',\n",
       " 'luckyid',\n",
       " 'usbpowered',\n",
       " 'namthe',\n",
       " 'hallorancertified',\n",
       " 'dvdmovie',\n",
       " 'musingsfootage',\n",
       " '2000signs',\n",
       " 'guantaacutenamo',\n",
       " 'uninterestingly',\n",
       " 'sexualphysical',\n",
       " 'pollanorganic',\n",
       " 'activelywhen',\n",
       " 'feldmarshall',\n",
       " 'gpsthe',\n",
       " 'verticalthe',\n",
       " 'guttenberg',\n",
       " 'needlelike',\n",
       " 'correctionspage',\n",
       " 'thereyes',\n",
       " 'waterproofdiscovering',\n",
       " '1013',\n",
       " 'control5',\n",
       " 'betterits',\n",
       " 'elsewhere3take',\n",
       " 'turturros',\n",
       " 'memoire',\n",
       " 'grime',\n",
       " 'grandscale',\n",
       " 'denominationalism',\n",
       " 'seam',\n",
       " 'mpeg4divx',\n",
       " 'sessionsmy',\n",
       " 'hoodie',\n",
       " 'tinkly',\n",
       " '34die',\n",
       " 'himgreat',\n",
       " '1000i',\n",
       " 'noodles',\n",
       " 'puller',\n",
       " 'likeabilityleah',\n",
       " 'allmany',\n",
       " 'prisonerthen',\n",
       " 'quandry',\n",
       " 'reverie',\n",
       " '10xs',\n",
       " 'eurabia',\n",
       " 'carman',\n",
       " 'aldridge',\n",
       " 'shimada',\n",
       " 'featurecooling',\n",
       " 'silvia',\n",
       " 'journeybug',\n",
       " 'quotcountry']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rare Words Removal\n",
    "Y = -100\n",
    "rareWordsTrainingFull = pd.Series(' '.join(trainingFull['Review']).split()).value_counts()[Y:]\n",
    "rareWordsTestingFull = pd.Series(' '.join(testingFull['Review']).split()).value_counts()[Y:]\n",
    "rareWordsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep to remove RARE and fairly common Words Method 1, THE FOLLOWING is for FULL, you can change the names to small or half\n",
    "freqTotalFull = list(freqTotalFull.index)\n",
    "freqTestingTotalFull = list(freqTestingTotalFull.index)\n",
    "rareWordsTrainingFull = list(rareWordsTestingFull.index)\n",
    "rareWordsTestingFull = list(rareWordsTestingFull.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For convienence \n",
    "trainingFinal = trainingSmall\n",
    "testingFinal = testingSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21540    beginning ipad mini fit using days found rubbe...\n",
       "2713     boring repetitive annoying started hope main c...\n",
       "6404     happens everytime pick latest clancy pick open...\n",
       "21319    could given stars title best thing comparisons...\n",
       "2224     consider christianity scam christians united g...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingFinal['Review'] = trainingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTotalFull))\n",
    "trainingFinal['Review'] = trainingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTrainingFull))\n",
    "trainingFinal['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19789    caretaker short low budget affair hour 17 minu...\n",
       "17003    dealt amazon decade discussing thing ever done...\n",
       "5681     stained looked coffee covered back back cover ...\n",
       "18958    never received item never received refund didn...\n",
       "19657    bad poorly written waste buck google search re...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingFinal['Review'] = testingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTestingTotalFull))\n",
    "testingFinal['Review'] = testingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTestingFull))\n",
    "testingFinal['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('also', 10589),\n",
       " ('could', 10497),\n",
       " ('way', 10365),\n",
       " ('many', 9945),\n",
       " ('people', 8862),\n",
       " ('author', 8841),\n",
       " ('know', 8836),\n",
       " ('think', 8585),\n",
       " ('reading', 8581),\n",
       " ('life', 8508),\n",
       " ('see', 8492),\n",
       " ('didnt', 8325),\n",
       " ('two', 8303),\n",
       " ('make', 8255),\n",
       " ('little', 8179),\n",
       " ('series', 8004),\n",
       " ('new', 7732),\n",
       " ('im', 7706),\n",
       " ('never', 7633),\n",
       " ('better', 7191),\n",
       " ('film', 7135),\n",
       " ('back', 6942),\n",
       " ('find', 6829),\n",
       " ('work', 6744),\n",
       " ('want', 6741),\n",
       " ('character', 6677),\n",
       " ('still', 6422),\n",
       " ('end', 6379),\n",
       " ('made', 6331),\n",
       " ('found', 6269),\n",
       " ('going', 6181),\n",
       " ('say', 6086),\n",
       " ('another', 5945),\n",
       " ('something', 5828),\n",
       " ('bad', 5790),\n",
       " ('things', 5713),\n",
       " ('go', 5616),\n",
       " ('use', 5608),\n",
       " ('best', 5583),\n",
       " ('cant', 5572),\n",
       " ('got', 5563),\n",
       " ('doesnt', 5434),\n",
       " ('years', 5400),\n",
       " ('us', 5399),\n",
       " ('world', 5357),\n",
       " ('plot', 5296),\n",
       " ('lot', 5292),\n",
       " ('novel', 5195),\n",
       " ('written', 5187),\n",
       " ('writing', 5128)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Classifier Part/Bag O WORDS\n",
    "# Only get the Reviews and tokenize\n",
    "TR = trainingFinal.to_csv(header=None, index = False)\n",
    "tokenizedList = nltk.word_tokenize(TR)\n",
    "common = nltk.FreqDist(tokenizedList)\n",
    "# 50 most common in this total distribution\n",
    "common.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Frequent Words for Other Ratings\n",
    "freqR1 = pd.Series(' '.join(dataTrainR1['Review']).split()).value_counts()[:x]\n",
    "freqR2 = pd.Series(' '.join(dataTrainR2['Review']).split()).value_counts()[:x]\n",
    "freqR3 = pd.Series(' '.join(dataTrainR3['Review']).split()).value_counts()[:x]\n",
    "freqR4 = pd.Series(' '.join(dataTrainR4['Review']).split()).value_counts()[:x]\n",
    "freqR5 = pd.Series(' '.join(dataTrainR5['Review']).split()).value_counts()[:x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creation of Bag of words for those classifiers\n",
    "TR1 = dataTrainR1.to_csv(header=None, index = False)\n",
    "tokenizedListR1 = nltk.word_tokenize(TR1)\n",
    "commonR1 = nltk.FreqDist(tokenizedListR1)\n",
    "\n",
    "TR2 = dataTrainR2.to_csv(header=None, index = False)\n",
    "tokenizedListR2 = nltk.word_tokenize(TR2)\n",
    "commonR2 = nltk.FreqDist(tokenizedListR2)\n",
    "\n",
    "TR3 = dataTrainR3.to_csv(header=None, index = False)\n",
    "tokenizedListR3 = nltk.word_tokenize(TR3)\n",
    "commonR3 = nltk.FreqDist(tokenizedListR3)\n",
    "\n",
    "TR4 = dataTrainR4.to_csv(header=None, index = False)\n",
    "tokenizedListR4 = nltk.word_tokenize(TR4)\n",
    "commonR4 = nltk.FreqDist(tokenizedListR4)\n",
    "\n",
    "TR5 = dataTrainR5.to_csv(header=None, index = False)\n",
    "tokenizedListR5 = nltk.word_tokenize(TR5)\n",
    "commonR5 = nltk.FreqDist(tokenizedListR5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This number is just taken randomly for no apparent reason do for all lists then combine them for getting the most common words\n",
    "WordCnt = 2500\n",
    "mostCommonR1 = commonR1.most_common(WordCnt)\n",
    "tempR1 = [x[0] for x in mostCommonR1]\n",
    "mostCommonWordsR1 = set(tempR1)\n",
    "\n",
    "mostCommonR2 = commonR2.most_common(WordCnt)\n",
    "tempR2 = [x[0] for x in mostCommonR2]\n",
    "mostCommonWordsR2 = set(tempR2)\n",
    "\n",
    "mostCommonR3 = commonR3.most_common(WordCnt)\n",
    "tempR3 = [x[0] for x in mostCommonR3]\n",
    "mostCommonWordsR3 = set(tempR3)\n",
    "\n",
    "mostCommonR4 = commonR4.most_common(WordCnt)\n",
    "tempR4 = [x[0] for x in mostCommonR4]\n",
    "mostCommonWordsR4 = set(tempR4)\n",
    "\n",
    "mostCommonR5 = commonR5.most_common(WordCnt)\n",
    "tempR5 = [x[0] for x in mostCommonR5]\n",
    "mostCommonWordsR5 = set(tempR5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3196"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining Multiple Bag of Words Sets into One\n",
    "mostCommonWordsTotal = mostCommonWordsR1\n",
    "mostCommonWordsTotal.update(mostCommonWordsR2)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR3)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR4)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR5)\n",
    "len(mostCommonWordsTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-29 16:32:59.299349\n"
     ]
    }
   ],
   "source": [
    "# Making Weights and see if sentences for those reviews have bag O words present in them\n",
    "import datetime\n",
    "# Just to See When Finished\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT)) \n",
    "ResultTUPOverall = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in trainingFinal.iterrows()]\n",
    "print (str(currentDT)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifierOverall = nltk.NaiveBayesClassifier.train(ResultTUPOverall)\n",
    "classifierOverall.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparing with Test data \n",
    "ResultTestList = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in testingFinal.iterrows()]\n",
    "# after this feed into accruacy classifier\n",
    "print(nltk.classify.accuracy(classifierOverall, ResultTestList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"classifierSmall.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierSmall, file)\n",
    "\n",
    "pkl_filename = \"classifierOverall.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierOverall, file)\n",
    "\n",
    "pkl_filename = \"classifierHalf.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierHalf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining Multiple Bag of Words Sets into One\n",
    "mostCommonWordsTotal = mostCommonWordsR1\n",
    "mostCommonWordsTotal.update(mostCommonWordsR2)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR3)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR4)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR5)\n",
    "len(mostCommonWordsTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Half DataSet\n",
    "TRHalf = trainingHalf.to_csv(header=None, index = False)\n",
    "tokenizedListHalf = nltk.word_tokenize(TR)\n",
    "commonHalf = nltk.FreqDist(tokenizedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is for small\n",
    "ResultTupOD = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in training.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                1 : 4      =     26.4 : 1.0\n",
      "                  highly = True                5 : 3      =      4.7 : 1.0\n",
      "                   money = True                1 : 4      =      4.5 : 1.0\n",
      "               wonderful = True                5 : 1      =      4.0 : 1.0\n",
      "                   loved = True                5 : 1      =      3.9 : 1.0\n",
      "                 enjoyed = True                3 : 1      =      3.8 : 1.0\n",
      "                     bad = True                1 : 4      =      3.7 : 1.0\n",
      "                   liked = True                3 : 1      =      3.6 : 1.0\n",
      "                   didnt = True                2 : 4      =      3.3 : 1.0\n",
      "                   wasnt = True                2 : 4      =      3.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier SMALL\n",
    "classifierSmall = nltk.NaiveBayesClassifier.train(ResultTupOD)\n",
    "classifierSmall.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what a shame i was really looking forward to a period drama worthy of source material instead i got artsyfartsy rubbish the actors do fine but they seem out of place in this strange conglomeration of stage decorations which only seems to be there for one of two reasons a to show how ohsooriginal the filmmakers are not or b because they didnt want to spend a ton of money on actual realistic scenes of the glorious gorgeous 19th century russia and its high society even though the movie obviously thinks itself ohsooriginal it really isnt how many times have you seen a variation of time stopping as two people are falling in love showing it from a different angle doesnt make it original last but not least anyone care to explain why kremlin which is in moscow is a recurring background for scenes that are supposed to be happening in st petersburg looks like they saved some money on consultants as well so is it epic yes an epic disappointment :  2\n",
      "True:  2  small:  2  half:  2\n"
     ]
    }
   ],
   "source": [
    "# Testing For Small and half\n",
    "# This will randomly select an object from dataset and compare it \n",
    "\n",
    "row = testing.sample(1)\n",
    "test_sentence = row.iloc[0]['Review']\n",
    "rating = row.iloc[0]['Rating']\n",
    "print(test_sentence, \": \", rating)\n",
    "\n",
    "test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in mostCommonWordsTotal}\n",
    "resultSmall = classifierSmall.classify(test_sent_features)\n",
    "resultHalf = classifierHalf.classify(test_sent_features)\n",
    "\n",
    "print(\"True: \" , rating , \" small: \", resultSmall, \" half: \", resultHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(classifierPickleHalf.classify(test_sent_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparing with Test data \n",
    "ResultTestList = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in testing.iterrows()]\n",
    "# after this feed into accruacy classifier\n",
    "print(nltk.classify.accuracy(classifierHalf, ResultTestList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  This is for Half\n",
    "ResultTupODHalf = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in trainingHalf.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                1 : 4      =     27.3 : 1.0\n",
      "                   money = True                1 : 4      =      4.5 : 1.0\n",
      "                  highly = True                5 : 2      =      4.4 : 1.0\n",
      "               wonderful = True                5 : 1      =      4.2 : 1.0\n",
      "                 enjoyed = True                3 : 1      =      3.9 : 1.0\n",
      "                   loved = True                5 : 1      =      3.8 : 1.0\n",
      "                   liked = True                3 : 1      =      3.7 : 1.0\n",
      "                     bad = True                1 : 4      =      3.4 : 1.0\n",
      "                    wait = True                5 : 3      =      3.3 : 1.0\n",
      "                     bit = True                3 : 1      =      3.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier Part\n",
    "classifierHalf = nltk.NaiveBayesClassifier.train(ResultTupODHalf)\n",
    "classifierHalf.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
