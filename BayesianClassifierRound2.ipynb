{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading in All data for all parsed/cleaned ratings\n",
    "dataTestR1 = pd.read_csv('parsed_data/testing_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTestR1.columns = [\"Review\"]\n",
    "dataTestR1.loc[:,'Rating'] = 1\n",
    "dataTestR2 = pd.read_csv('parsed_data/testing_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTestR2.columns = [\"Review\"]\n",
    "dataTestR2.loc[:,'Rating'] = 2\n",
    "dataTestR3 = pd.read_csv('parsed_data/testing_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTestR3.columns = [\"Review\"]\n",
    "dataTestR3.loc[:,'Rating'] = 3\n",
    "dataTestR4 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR4.columns = [\"Review\"]\n",
    "dataTestR4.loc[:,'Rating'] = 4\n",
    "dataTestR5 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR5.columns = [\"Review\"]\n",
    "dataTestR5.loc[:,'Rating'] = 5\n",
    "\n",
    "dataTrainR1 = pd.read_csv('parsed_data/training_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR1.columns = [\"Review\"]\n",
    "dataTrainR1.loc[:,'Rating'] = 1\n",
    "dataTrainR2 = pd.read_csv('parsed_data/training_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR2.columns = [\"Review\"]\n",
    "dataTrainR2.loc[:,'Rating'] = 2\n",
    "dataTrainR3 = pd.read_csv('parsed_data/training_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR3.columns = [\"Review\"]\n",
    "dataTrainR3.loc[:,'Rating'] = 3\n",
    "dataTrainR4 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR4.columns = [\"Review\"]\n",
    "dataTrainR4.loc[:,'Rating'] = 4\n",
    "dataTrainR5 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR5.columns = [\"Review\"]\n",
    "dataTrainR5.loc[:,'Rating'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as an avid reader i was not happy with this bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever since the action in the anita blake shift...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when historians go dumpster diving through the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im a big fan of michael crichton and the huge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i might have considered buying this ebook if t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  as an avid reader i was not happy with this bo...       1\n",
       "1  ever since the action in the anita blake shift...       1\n",
       "2  when historians go dumpster diving through the...       1\n",
       "3  im a big fan of michael crichton and the huge ...       1\n",
       "4  i might have considered buying this ebook if t...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small Sample Trial Run\n",
    "# training = dataTrainR1 + dataTrainR2 + dataTrainR3 + dataTrainR4 + dataTrainR5\n",
    "# testing = dataTestR1 + dataTestR2 + dataTestR3 + dataTestR4 + dataTestR5\n",
    "framesTrain = [dataTrainR1,dataTrainR2,dataTrainR3,dataTrainR4,dataTrainR5]\n",
    "framesTest = [dataTestR1, dataTestR2, dataTestR3 , dataTestR4 , dataTestR5]\n",
    "training = pd.concat(framesTrain)\n",
    "testing = pd.concat(framesTest)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_in_reviews(reviews):\n",
    "    all_words = []\n",
    "    for sentence, rating in zip(training.iloc[:,0], training.iloc[:,1]):\n",
    "        print(sentence, rating)\n",
    "        sentence = str(sentence)\n",
    "        all_words.extend(sentence)\n",
    "    return all_words\n",
    "def get_rating_features(reviewList):\n",
    "    reviewList = nltk.FreqDist(reviewList)\n",
    "    feature = reviewList.keys()\n",
    "    return feature\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' %word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingReviews = training[['Review']]\n",
    "# print(trainingReviews.to_csv(header=None, index=False))\n",
    "# TR = trainingReviews.to_csv(header=None, index = False) #For Method 2\n",
    "# for index, review in trainingReviews.iterrows():\n",
    "# common = nltk.FreqDist(trainingReviews)\n",
    "\n",
    "# Demo Below\n",
    "# training2 = training\n",
    "# training2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going Through All DataSets\n",
    "for index, row in training.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        training.drop(index, inplace=True)\n",
    "# Going through TrainR1\n",
    "for index, row in dataTrainR1.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR1.drop(index, inplace=True)\n",
    "# Going Through TrainR2\n",
    "for index, row in dataTrainR2.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR2.drop(index, inplace=True)\n",
    "# Going Through TrainR3\n",
    "for index, row in dataTrainR3.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR3.drop(index, inplace=True)\n",
    "# Going Through TrainR4\n",
    "for index, row in dataTrainR4.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR4.drop(index, inplace=True)\n",
    "# Going Through Train R5\n",
    "for index, row in dataTrainR5.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR5.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy book seemed rushed character...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1 of Removing Stop words with data frame\n",
    "stop = stopwords.words('english')\n",
    "# Could add more into Stop words\n",
    "training['Review'] = training['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "training['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avid reader happy book seemed rushed character...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever since action anita blake shifted action s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>historians go dumpster diving cultural effluvi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im big fan michael crichton huge controversy l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>might considered buying ebook look inside show...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  avid reader happy book seemed rushed character...       1\n",
       "1  ever since action anita blake shifted action s...       1\n",
       "2  historians go dumpster diving cultural effluvi...       1\n",
       "3  im big fan michael crichton huge controversy l...       1\n",
       "4  might considered buying ebook look inside show...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing for the Others\n",
    "dataTrainR1['Review'] = dataTrainR1['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR2['Review'] = dataTrainR2['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR3['Review'] = dataTrainR3['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR4['Review'] = dataTrainR4['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR5['Review'] = dataTrainR5['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 Tokenize then remove stop words from a sentence input\n",
    "# Tokenize The String Listings\n",
    "# tokenizedList = nltk.word_tokenize(TR)\n",
    "# common = nltk.FreqDist(tokenizedList)\n",
    "# Setting StopWords to those that are in english\n",
    "# stop = stopwords.words('english')\n",
    "# common.most_common(50)\n",
    "# Removing StopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          302098\n",
       "one           152305\n",
       "like          121311\n",
       "read          119507\n",
       "story         109497\n",
       "would          99940\n",
       "good           85693\n",
       "really         77983\n",
       "get            72910\n",
       "time           71818\n",
       "much           71251\n",
       "even           66033\n",
       "first          63016\n",
       "characters     62089\n",
       "great          61991\n",
       "love           59676\n",
       "well           59244\n",
       "books          57676\n",
       "dont           55605\n",
       "movie          55050\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words can or won't do this\n",
    "x = 20\n",
    "freqTotal = pd.Series(' '.join(training['Review']).split()).value_counts()[:x]\n",
    "freqTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Frequent Words for Other Ratings\n",
    "freqR1 = pd.Series(' '.join(dataTrainR1['Review']).split()).value_counts()[:x]\n",
    "freqR2 = pd.Series(' '.join(dataTrainR2['Review']).split()).value_counts()[:x]\n",
    "freqR3 = pd.Series(' '.join(dataTrainR3['Review']).split()).value_counts()[:x]\n",
    "freqR4 = pd.Series(' '.join(dataTrainR4['Review']).split()).value_counts()[:x]\n",
    "freqR5 = pd.Series(' '.join(dataTrainR5['Review']).split()).value_counts()[:x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zeroannoyance            1\n",
       "wps10                    1\n",
       "notwithstandingand       1\n",
       "soliah                   1\n",
       "factask                  1\n",
       "dailyprossmall           1\n",
       "failurefalse             1\n",
       "outcastapparently        1\n",
       "itdanas                  1\n",
       "abolishedthe             1\n",
       "anothersituations        1\n",
       "lowprofit                1\n",
       "afol                     1\n",
       "2012so                   1\n",
       "bro3                     1\n",
       "everyoneanyway           1\n",
       "usr8700                  1\n",
       "cohesivenessoneness      1\n",
       "onequestion              1\n",
       "needsit                  1\n",
       "orbitals                 1\n",
       "kanai                    1\n",
       "972                      1\n",
       "punchyneurotic           1\n",
       "mastersit                1\n",
       "reflectionsreadings      1\n",
       "futurefollow             1\n",
       "riskiershopgirl          1\n",
       "extrasuperglorious       1\n",
       "quotpopcorn              1\n",
       "                        ..\n",
       "hbkworst                 1\n",
       "testamentin              1\n",
       "preterit                 1\n",
       "gatsbyquot               1\n",
       "38another                1\n",
       "cardo                    1\n",
       "cringeing                1\n",
       "pantherhipped            1\n",
       "fameand                  1\n",
       "revealthis               1\n",
       "cdbluraydvdgame          1\n",
       "depthtwo                 1\n",
       "popsiclesnever           1\n",
       "asilverstone             1\n",
       "povinterestingly         1\n",
       "panfried                 1\n",
       "swingingshe              1\n",
       "beganto                  1\n",
       "westmezlekia             1\n",
       "articlebut               1\n",
       "appsbecause              1\n",
       "imposterrumanian         1\n",
       "kildas                   1\n",
       "pandr                    1\n",
       "lilliput2when            1\n",
       "quotscientistsquot       1\n",
       "redheadi                 1\n",
       "2003hulk                 1\n",
       "networksignificant       1\n",
       "512gbimpressionsnoisy    1\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rare Words Removal\n",
    "Y = -100\n",
    "rareWordsTotal = pd.Series(' '.join(training['Review']).split()).value_counts()[Y:]\n",
    "rareWordsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do it for the others pROBABLY Unecessary for comparision\n",
    "rareWordsR1 = pd.Series(' '.join(dataTrainR1['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR2 = pd.Series(' '.join(dataTrainR2['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR3 = pd.Series(' '.join(dataTrainR3['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR4 = pd.Series(' '.join(dataTrainR4['Review']).split()).value_counts()[Y:]\n",
    "rareWordsR5 = pd.Series(' '.join(dataTrainR5['Review']).split()).value_counts()[Y:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of both rare words and common words part 1\n",
    "freqTotal = list(freqTotal.index)\n",
    "rareWordsTota = list(rareWordsTotal.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy seemed rushed history could ...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of both rare and common words part 2\n",
    "training['Review'] = training['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTotal))\n",
    "# training2['Review'].head()\n",
    "training['Review'] = training['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTotal))\n",
    "training['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier Part\n",
    "# Only get the Reviews and tokenize\n",
    "TR = training.to_csv(header=None, index = False)\n",
    "tokenizedList = nltk.word_tokenize(TR)\n",
    "common = nltk.FreqDist(tokenizedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('also', 53172),\n",
       " ('could', 52617),\n",
       " ('way', 51700),\n",
       " ('many', 50124),\n",
       " ('people', 45033),\n",
       " ('know', 44395),\n",
       " ('author', 44138),\n",
       " ('reading', 44129),\n",
       " ('think', 42976),\n",
       " ('life', 42407),\n",
       " ('see', 41716),\n",
       " ('two', 41702),\n",
       " ('little', 40946),\n",
       " ('didnt', 40946),\n",
       " ('make', 40924),\n",
       " ('series', 40866),\n",
       " ('new', 39079),\n",
       " ('never', 38800),\n",
       " ('im', 38794),\n",
       " ('better', 36187),\n",
       " ('film', 34767),\n",
       " ('back', 34403),\n",
       " ('find', 34342),\n",
       " ('work', 34125),\n",
       " ('want', 34115),\n",
       " ('character', 33295),\n",
       " ('end', 32713),\n",
       " ('still', 31908),\n",
       " ('made', 31886),\n",
       " ('found', 31043),\n",
       " ('say', 30889),\n",
       " ('going', 30445),\n",
       " ('another', 29553),\n",
       " ('something', 29429),\n",
       " ('bad', 28705),\n",
       " ('go', 28613),\n",
       " ('things', 28429),\n",
       " ('best', 27970),\n",
       " ('got', 27963),\n",
       " ('use', 27758),\n",
       " ('cant', 27718),\n",
       " ('doesnt', 27326),\n",
       " ('plot', 27154),\n",
       " ('lot', 27079),\n",
       " ('world', 26922),\n",
       " ('us', 26493),\n",
       " ('years', 26390),\n",
       " ('novel', 26209),\n",
       " ('writing', 26020),\n",
       " ('interesting', 25778)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 most common in this total distribution\n",
    "common.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do for the others\n",
    "TR1 = dataTrainR1.to_csv(header=None, index = False)\n",
    "tokenizedListR1 = nltk.word_tokenize(TR1)\n",
    "commonR1 = nltk.FreqDist(tokenizedListR1)\n",
    "\n",
    "TR2 = dataTrainR2.to_csv(header=None, index = False)\n",
    "tokenizedListR2 = nltk.word_tokenize(TR2)\n",
    "commonR2 = nltk.FreqDist(tokenizedListR2)\n",
    "\n",
    "TR3 = dataTrainR3.to_csv(header=None, index = False)\n",
    "tokenizedListR3 = nltk.word_tokenize(TR3)\n",
    "commonR3 = nltk.FreqDist(tokenizedListR3)\n",
    "\n",
    "TR4 = dataTrainR4.to_csv(header=None, index = False)\n",
    "tokenizedListR4 = nltk.word_tokenize(TR4)\n",
    "commonR4 = nltk.FreqDist(tokenizedListR4)\n",
    "\n",
    "TR5 = dataTrainR5.to_csv(header=None, index = False)\n",
    "tokenizedListR5 = nltk.word_tokenize(TR5)\n",
    "commonR5 = nltk.FreqDist(tokenizedListR5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 52910),\n",
       " ('one', 26769),\n",
       " ('read', 23688),\n",
       " ('story', 19635),\n",
       " ('great', 17338),\n",
       " ('like', 16586),\n",
       " ('love', 16338),\n",
       " ('good', 13403),\n",
       " ('would', 13169),\n",
       " ('well', 12643),\n",
       " ('really', 11877),\n",
       " ('time', 11745),\n",
       " ('get', 10884),\n",
       " ('books', 10845),\n",
       " ('first', 10694),\n",
       " ('also', 10318),\n",
       " ('characters', 10305),\n",
       " ('much', 10210),\n",
       " ('life', 10102),\n",
       " ('even', 9616),\n",
       " ('series', 9435),\n",
       " ('way', 9040),\n",
       " ('many', 8910),\n",
       " ('reading', 8638),\n",
       " ('new', 7799),\n",
       " ('could', 7553),\n",
       " ('see', 7543),\n",
       " ('know', 7462),\n",
       " ('people', 7405),\n",
       " ('movie', 7093),\n",
       " ('two', 6781),\n",
       " ('loved', 6743),\n",
       " ('best', 6735),\n",
       " ('dont', 6721),\n",
       " ('make', 6622),\n",
       " ('want', 6506),\n",
       " ('find', 6412),\n",
       " ('author', 6327),\n",
       " ('little', 6285),\n",
       " ('never', 6279),\n",
       " ('think', 6239),\n",
       " ('world', 5790),\n",
       " ('im', 5767),\n",
       " ('cant', 5719),\n",
       " ('us', 5624),\n",
       " ('back', 5609),\n",
       " ('still', 5477),\n",
       " ('work', 5433),\n",
       " ('years', 5399),\n",
       " ('recommend', 5370)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommonR5 = commonR5.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official Continuation of full datasize\n",
    "# training.head()\n",
    "all_wordsBag = set(word.lower() for passage in training['Review'] for word in nltk.word_tokenize(passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-749f90b1e4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mResultTupOD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_wordsBag\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifierBae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResultTupOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifierBae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-749f90b1e4f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mResultTupOD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_wordsBag\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifierBae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResultTupOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifierBae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-749f90b1e4f3>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mResultTupOD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_wordsBag\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifierBae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResultTupOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifierBae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 132\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# We are not using CONTRACTIONS4 since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ResultTupOD = [({word: (word in nltk.word_tokenize(row['Review'])) for word in all_wordsBag}, row['Rating']) for index,row in training.iterrows()]\n",
    "classifierBae = nltk.NaiveBayesClassifier.train(ResultTupOD)\n",
    "classifierBae.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read many books general topic thought onewas w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loved 34the girl dragon tattoo34 series though...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kelsey grammar cant take seriously hard ass st...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty disappointed book expecting bowled lang...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one shahs books popularized sufi path america ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love brides kindred series little disappointed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prince jorg mission revenge revenge fallen mot...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone familiar line merchant venice question ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>til death us part compilation 15 short stories...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis seemed promising clear mind work spen...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  read many books general topic thought onewas w...       2\n",
       "1  loved 34the girl dragon tattoo34 series though...       2\n",
       "2  kelsey grammar cant take seriously hard ass st...       2\n",
       "3  pretty disappointed book expecting bowled lang...       2\n",
       "4  one shahs books popularized sufi path america ...       2\n",
       "0  love brides kindred series little disappointed...       3\n",
       "1  prince jorg mission revenge revenge fallen mot...       3\n",
       "2  anyone familiar line merchant venice question ...       3\n",
       "3  til death us part compilation 15 short stories...       3\n",
       "4  synopsis seemed promising clear mind work spen...       3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing/Trial An Error Part\n",
    "# tempTrainR1 = dataTrainR1[:2].to_csv(header=None, index = False)\n",
    "tempTrainR1 = dataTrainR1[:2]\n",
    "tempTrainR2 = dataTrainR2[:5]\n",
    "tempTrainR3 = dataTrainR3[:5]\n",
    "framesTempTrain = [tempTrainR2, tempTrainR3]\n",
    "\n",
    "tempTrainTot = pd.concat(framesTempTrain)\n",
    "# tokenListTrainR1 = nltk.word_tokenize(tempTrainR1)\n",
    "# tokenListTrainR1\n",
    "# tempTrainTot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempTrainR1['Review'] = tempTrainR1.apply(lambda row: nltk.word_tokenize(row['Review']), axis=1) Consider Doing Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'15',\n",
       " '34',\n",
       " '34the',\n",
       " '82038203me',\n",
       " '911',\n",
       " 'abruptly',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'additionthe',\n",
       " 'admit',\n",
       " 'adults',\n",
       " 'advocate',\n",
       " 'afraid',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'ali',\n",
       " 'allies',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'alongside',\n",
       " 'also',\n",
       " 'alternates',\n",
       " 'although',\n",
       " 'america',\n",
       " 'anarchistthis',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'apart',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appeared',\n",
       " 'around',\n",
       " 'ass',\n",
       " 'assist',\n",
       " 'attempt',\n",
       " 'audience',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'autobiographical',\n",
       " 'background',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'beg',\n",
       " 'ben',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'blockbuster',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bordering',\n",
       " 'bored',\n",
       " 'bowled',\n",
       " 'box',\n",
       " 'braggingthere',\n",
       " 'brides',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'building',\n",
       " 'bumming',\n",
       " 'buried',\n",
       " 'c',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'certainly',\n",
       " 'challenging',\n",
       " 'champions',\n",
       " 'change',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'charmed',\n",
       " 'chemistry',\n",
       " 'chicken',\n",
       " 'childish',\n",
       " 'chore',\n",
       " 'citys',\n",
       " 'claimed',\n",
       " 'class',\n",
       " 'classprofessional',\n",
       " 'clear',\n",
       " 'cliches',\n",
       " 'closer',\n",
       " 'coldhearted',\n",
       " 'comedy',\n",
       " 'comical',\n",
       " 'commitmentbut',\n",
       " 'compilation',\n",
       " 'complains',\n",
       " 'concur',\n",
       " 'conditions',\n",
       " 'confused',\n",
       " 'considered',\n",
       " 'constantly',\n",
       " 'continue',\n",
       " 'contrived',\n",
       " 'convinced',\n",
       " 'corrosive',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'counter',\n",
       " 'cover',\n",
       " 'cringethis',\n",
       " 'culture',\n",
       " 'current',\n",
       " 'dawson',\n",
       " 'death',\n",
       " 'debut',\n",
       " 'dedicated',\n",
       " 'defiantly',\n",
       " 'degradation',\n",
       " 'depression',\n",
       " 'describe',\n",
       " 'deserving',\n",
       " 'desperately',\n",
       " 'despite',\n",
       " 'destruction',\n",
       " 'deter',\n",
       " 'didnt',\n",
       " 'difficult',\n",
       " 'disabled',\n",
       " 'disappointed',\n",
       " 'dissecting',\n",
       " 'disturbing',\n",
       " 'diverse',\n",
       " 'divorce',\n",
       " 'doctrine',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'dont',\n",
       " 'doors',\n",
       " 'draft',\n",
       " 'dragon',\n",
       " 'drama',\n",
       " 'dramatic',\n",
       " 'driven',\n",
       " 'editing',\n",
       " 'effort',\n",
       " 'either',\n",
       " 'elicit',\n",
       " 'elseultimately',\n",
       " 'emergency',\n",
       " 'emily',\n",
       " 'emotional',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enough',\n",
       " 'entails',\n",
       " 'entirely',\n",
       " 'erroneously',\n",
       " 'especially',\n",
       " 'essence',\n",
       " 'evening',\n",
       " 'evenings',\n",
       " 'events',\n",
       " 'ever',\n",
       " 'everything',\n",
       " 'exist',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'expecting',\n",
       " 'experience',\n",
       " 'eyes',\n",
       " 'fallen',\n",
       " 'falsely',\n",
       " 'familiar',\n",
       " 'fanny',\n",
       " 'fantasized',\n",
       " 'far',\n",
       " 'favorite',\n",
       " 'fed',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'film',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'flesh',\n",
       " 'follow',\n",
       " 'followed',\n",
       " 'food',\n",
       " 'forced',\n",
       " 'formulated',\n",
       " 'forth',\n",
       " 'forward',\n",
       " 'four',\n",
       " 'fourteen',\n",
       " 'frager',\n",
       " 'franklin',\n",
       " 'freedom',\n",
       " 'front',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'genre',\n",
       " 'get',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'good',\n",
       " 'goodreads',\n",
       " 'grammar',\n",
       " 'great',\n",
       " 'group',\n",
       " 'guess',\n",
       " 'guessed',\n",
       " 'guts',\n",
       " 'guy',\n",
       " 'hand',\n",
       " 'hang',\n",
       " 'haphazardly',\n",
       " 'happy',\n",
       " 'happyness',\n",
       " 'hard',\n",
       " 'harrelson',\n",
       " 'hating',\n",
       " 'heart',\n",
       " 'heartbroken',\n",
       " 'hearts',\n",
       " 'heroes',\n",
       " 'himselfthe',\n",
       " 'hippie',\n",
       " 'hm',\n",
       " 'holiday',\n",
       " 'honest',\n",
       " 'however',\n",
       " 'idea',\n",
       " 'ideal',\n",
       " 'identify',\n",
       " 'idries',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'imagine',\n",
       " 'importance',\n",
       " 'inconsistencies',\n",
       " 'individual',\n",
       " 'inevitable',\n",
       " 'info',\n",
       " 'ingredients',\n",
       " 'inspire',\n",
       " 'interesting',\n",
       " 'introduction',\n",
       " 'irony',\n",
       " 'irs',\n",
       " 'jerk',\n",
       " 'jk',\n",
       " 'jointshilarious',\n",
       " 'jorg',\n",
       " 'journey',\n",
       " 'judgment',\n",
       " 'kate',\n",
       " 'kates',\n",
       " 'keeping',\n",
       " 'keeps',\n",
       " 'kelsey',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kindred',\n",
       " 'knows',\n",
       " 'lacks',\n",
       " 'language',\n",
       " 'lapse',\n",
       " 'lawyer',\n",
       " 'leaderthe',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'letting',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'line',\n",
       " 'literature',\n",
       " 'little',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovein',\n",
       " 'lucienewbooksonmyselvesblogspotfr',\n",
       " 'made',\n",
       " 'magic',\n",
       " 'main',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'make',\n",
       " 'many',\n",
       " 'mark',\n",
       " 'marriage',\n",
       " 'master',\n",
       " 'mean',\n",
       " 'mebut',\n",
       " 'meet',\n",
       " 'merchant',\n",
       " 'middle',\n",
       " 'miller',\n",
       " 'mind',\n",
       " 'minebut',\n",
       " 'minimum',\n",
       " 'miserable',\n",
       " 'mission',\n",
       " 'momentary',\n",
       " 'moments',\n",
       " 'money',\n",
       " 'morning34',\n",
       " 'mostly',\n",
       " 'mother',\n",
       " 'motivations',\n",
       " 'mouththe',\n",
       " 'movie',\n",
       " 'mr',\n",
       " 'much',\n",
       " 'must',\n",
       " 'muzaffer',\n",
       " 'mystery',\n",
       " 'naturally',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'newage',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nihilism',\n",
       " 'nonplot',\n",
       " 'not',\n",
       " 'novel',\n",
       " 'number',\n",
       " 'numerous',\n",
       " 'office',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'one',\n",
       " 'onedimensional',\n",
       " 'onewas',\n",
       " 'operate',\n",
       " 'opinion',\n",
       " 'order',\n",
       " 'orirshad',\n",
       " 'otr',\n",
       " 'outer',\n",
       " 'ozak',\n",
       " 'palpable',\n",
       " 'paris',\n",
       " 'part',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'passion',\n",
       " 'past',\n",
       " 'path',\n",
       " 'pay',\n",
       " 'payday',\n",
       " 'pedestrian',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'personalitiesthe',\n",
       " 'philosophical',\n",
       " 'philosophize',\n",
       " 'philosophy',\n",
       " 'pick',\n",
       " 'place',\n",
       " 'placed',\n",
       " 'plain',\n",
       " 'play',\n",
       " 'played',\n",
       " 'plays',\n",
       " 'plot',\n",
       " 'point',\n",
       " 'polished',\n",
       " 'popularized',\n",
       " 'posa',\n",
       " 'position',\n",
       " 'potty',\n",
       " 'pound',\n",
       " 'predict',\n",
       " 'present',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'prince',\n",
       " 'princely',\n",
       " 'promising',\n",
       " 'propelling',\n",
       " 'prove',\n",
       " 'pursuit',\n",
       " 'pushes',\n",
       " 'quest',\n",
       " 'question',\n",
       " 'rather',\n",
       " 'rays',\n",
       " 'reactions',\n",
       " 'read',\n",
       " 'readersso',\n",
       " 'reading',\n",
       " 'realism',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'recommend',\n",
       " 'redemptive',\n",
       " 'refined',\n",
       " 'regardless',\n",
       " 'regret',\n",
       " 'relationships',\n",
       " 'report',\n",
       " 'represented',\n",
       " 'requested',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'resulting',\n",
       " 'revenge',\n",
       " 'reviews',\n",
       " 'revolves',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'role',\n",
       " 'romance',\n",
       " 'romantic',\n",
       " 'rosario',\n",
       " 'rough',\n",
       " 'roving',\n",
       " 'rummage',\n",
       " 'running',\n",
       " 'sacrifice',\n",
       " 'sainthood',\n",
       " 'sale',\n",
       " 'salvation',\n",
       " 'satisfactory',\n",
       " 'scandinavian',\n",
       " 'scholarship',\n",
       " 'score',\n",
       " 'screen',\n",
       " 'scripting',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'seemed',\n",
       " 'seems',\n",
       " 'sees',\n",
       " 'self',\n",
       " 'selfabsorbedwill',\n",
       " 'selfproclaimed',\n",
       " 'sell',\n",
       " 'sellsword',\n",
       " 'sensual',\n",
       " 'sequel',\n",
       " 'serie',\n",
       " 'series',\n",
       " 'seriously',\n",
       " 'set',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'sex',\n",
       " 'sexologist',\n",
       " 'sexual',\n",
       " 'shah',\n",
       " 'shahs',\n",
       " 'short',\n",
       " 'sighing',\n",
       " 'since',\n",
       " 'situations',\n",
       " 'six',\n",
       " 'slade',\n",
       " 'sleep',\n",
       " 'slightly',\n",
       " 'smile',\n",
       " 'smileadmittedly',\n",
       " 'smith',\n",
       " 'smiths',\n",
       " 'smok',\n",
       " 'snippets',\n",
       " 'society',\n",
       " 'sojourn',\n",
       " 'something',\n",
       " 'sorry',\n",
       " 'soul',\n",
       " 'souls',\n",
       " 'speaking',\n",
       " 'speech',\n",
       " 'spend',\n",
       " 'spiritual',\n",
       " 'stability',\n",
       " 'start',\n",
       " 'started',\n",
       " 'staying',\n",
       " 'stop',\n",
       " 'stories',\n",
       " 'story',\n",
       " 'stripped',\n",
       " 'strong',\n",
       " 'struggle',\n",
       " 'successful',\n",
       " 'suffuses',\n",
       " 'sufi',\n",
       " 'sufism',\n",
       " 'suicide',\n",
       " 'summertime',\n",
       " 'supernatural',\n",
       " 'support',\n",
       " 'supporting',\n",
       " 'surprised',\n",
       " 'swords',\n",
       " 'symbol',\n",
       " 'synopsis',\n",
       " 'system',\n",
       " 'take',\n",
       " 'tattoo34',\n",
       " 'taut',\n",
       " 'tearing',\n",
       " 'template',\n",
       " 'ten',\n",
       " 'tends',\n",
       " 'theni',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thirty',\n",
       " 'thomas',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'threw',\n",
       " 'tidbits',\n",
       " 'til',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'titles',\n",
       " 'toc',\n",
       " 'toggles',\n",
       " 'topic',\n",
       " 'touch',\n",
       " 'towards',\n",
       " 'transplant',\n",
       " 'traumatic',\n",
       " 'travel',\n",
       " 'true',\n",
       " 'try',\n",
       " 'turns',\n",
       " 'two',\n",
       " 'ugly',\n",
       " 'underlying',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unrealistic',\n",
       " 'unveiled',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'variably',\n",
       " 'venice',\n",
       " 'viewed',\n",
       " 'viewer',\n",
       " 'viewing',\n",
       " 'violent',\n",
       " 'virgin',\n",
       " 'visceral',\n",
       " 'vitriolic',\n",
       " 'walked',\n",
       " 'want',\n",
       " 'wasnt',\n",
       " 'watered',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'well',\n",
       " 'westernized',\n",
       " 'white',\n",
       " 'whites',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'woody',\n",
       " 'word',\n",
       " 'work',\n",
       " 'works',\n",
       " 'worth',\n",
       " 'worthy',\n",
       " 'would',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wry',\n",
       " 'years',\n",
       " 'young'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method2\n",
    "# tup = tempTrainR2['Review'][0]\n",
    "# type(tup)\n",
    "all_wordsFinal = set(word.lower() for passage in tempTrainTot['Review'] for word in nltk.word_tokenize(passage))\n",
    "# all_wordsR3 = set(word.lower() for passage in tempTrainR3['Review'] for word in nltk.word_tokenize(passage))\n",
    "# all_wordsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  little = False               2 : 3      =      3.0 : 1.0\n",
      "                  little = True                3 : 2      =      3.0 : 1.0\n",
      "                 reading = True                3 : 2      =      2.3 : 1.0\n",
      "                   story = False               2 : 3      =      2.3 : 1.0\n",
      "                  really = True                3 : 2      =      2.3 : 1.0\n",
      "                  really = False               2 : 3      =      1.8 : 1.0\n",
      "                 reading = False               2 : 3      =      1.8 : 1.0\n",
      "                   story = True                3 : 2      =      1.8 : 1.0\n",
      "                    read = True                2 : 3      =      1.7 : 1.0\n",
      "            disappointed = True                3 : 2      =      1.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# t = [({word: (word in nltk.word_tokenize(x[0])) for word in all_wordsSample}, x[1]) for x in trainSample]\n",
    "# trialR2\n",
    "#       {word: (word in nltk.word_tokenize(x[0])) for word in all_wordsSample}\n",
    "#     for word in all_wordsSample\n",
    "# Make dictionary \n",
    "# word : word in nltk.word_tokenize(x[0]))\n",
    "\n",
    "# This will give you a tuple of dictionary features, basically saying if this is present or not and what category it is in\n",
    "temp = [({word: (word in nltk.word_tokenize(row['Review'])) for word in all_wordsR2}, row['Rating']) for index,row in tempTrainTot.iterrows()]\n",
    "\n",
    "# x[0] should be the review\n",
    "# x[1] should be the rating\n",
    "# for index,row in tempTrainR2.iterrows():\n",
    "#     print(row['Review'], \" : \", row['Rating'])\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(temp)\n",
    "classifier2.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tryPredicting(testSentence):\n",
    "    test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in all_wordsFinal}\n",
    "    return classifier2.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# print(tryPredicting(\"\"))\n",
    "testingSample = \"\"\n",
    "outcomeRating = 1\n",
    "for index,row in dataTestR2.iterrows():\n",
    "    testingSample = row['Review']\n",
    "    outcomeRating = row['Rating']\n",
    "    break\n",
    "print(tryPredicting(testingSample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '.',\n",
       " 'about',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'an',\n",
       " 'awesome',\n",
       " 'beers',\n",
       " 'best',\n",
       " 'boss',\n",
       " 'ca',\n",
       " 'deal',\n",
       " 'do',\n",
       " 'enemy',\n",
       " 'feel',\n",
       " 'good',\n",
       " 'he',\n",
       " 'horrible',\n",
       " 'i',\n",
       " 'is',\n",
       " 'like',\n",
       " 'love',\n",
       " 'my',\n",
       " \"n't\",\n",
       " 'not',\n",
       " 'of',\n",
       " 'place',\n",
       " 'restaurant',\n",
       " 'sandwich',\n",
       " 'stuff',\n",
       " 'sworn',\n",
       " 'these',\n",
       " 'this',\n",
       " 'tired',\n",
       " 'very',\n",
       " 'view',\n",
       " 'what',\n",
       " 'with',\n",
       " 'work'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSample = [('I love this sandwich.', 'pos'),\n",
    "('This is an amazing place!', 'pos'),\n",
    "('I feel very good about these beers.', 'pos'),\n",
    "('This is my best work.', 'pos'),\n",
    "(\"What an awesome view\", 'pos'),\n",
    "('I do not like this restaurant', 'neg'),\n",
    "('I am tired of this stuff.', 'neg'),\n",
    "(\"I can't deal with this\", 'neg'),\n",
    "('He is my sworn enemy!', 'neg'),\n",
    "('My boss is horrible.', 'neg')]\n",
    "all_wordsSample = set(word.lower() for passage in trainSample for word in nltk.word_tokenize(passage[0]))\n",
    "# type(all_wordsSample)\n",
    "all_wordsSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in trainSample:\n",
    "#     print(x)\n",
    "#     for word in nltk.word_tokenize(x[0]):\n",
    "#         print(type(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': True,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': True,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': True,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': True,\n",
       "   'an': True,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': True,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': True,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': True,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': True,\n",
       "   'good': True,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': True,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': True,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': True,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': True,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': True},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': True,\n",
       "   'awesome': True,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': True,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'pos'),\n",
       " ({'!': False,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': True,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': True,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': True,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': True,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': True,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': True,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': True,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': True,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': False,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': True,\n",
       "   'deal': True,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': False,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": True,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': True,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': True,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': True,\n",
       "   '.': False,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': False,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': True,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': False,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': True,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': True,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg'),\n",
       " ({'!': False,\n",
       "   '.': True,\n",
       "   'about': False,\n",
       "   'am': False,\n",
       "   'amazing': False,\n",
       "   'an': False,\n",
       "   'awesome': False,\n",
       "   'beers': False,\n",
       "   'best': False,\n",
       "   'boss': True,\n",
       "   'ca': False,\n",
       "   'deal': False,\n",
       "   'do': False,\n",
       "   'enemy': False,\n",
       "   'feel': False,\n",
       "   'good': False,\n",
       "   'he': False,\n",
       "   'horrible': True,\n",
       "   'i': False,\n",
       "   'is': True,\n",
       "   'like': False,\n",
       "   'love': False,\n",
       "   'my': False,\n",
       "   \"n't\": False,\n",
       "   'not': False,\n",
       "   'of': False,\n",
       "   'place': False,\n",
       "   'restaurant': False,\n",
       "   'sandwich': False,\n",
       "   'stuff': False,\n",
       "   'sworn': False,\n",
       "   'these': False,\n",
       "   'this': False,\n",
       "   'tired': False,\n",
       "   'very': False,\n",
       "   'view': False,\n",
       "   'what': False,\n",
       "   'with': False,\n",
       "   'work': False},\n",
       "  'neg')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [({word: (word in nltk.word_tokenize(x[0])) for word in all_wordsSample}, x[1]) for x in trainSample]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    this = True              neg : pos    =      2.3 : 1.0\n",
      "                    this = False             pos : neg    =      1.8 : 1.0\n",
      "                      an = False             neg : pos    =      1.6 : 1.0\n",
      "                       . = True              pos : neg    =      1.4 : 1.0\n",
      "                       . = False             neg : pos    =      1.4 : 1.0\n",
      "              restaurant = False             pos : neg    =      1.2 : 1.0\n",
      "                     not = False             pos : neg    =      1.2 : 1.0\n",
      "                   these = False             neg : pos    =      1.2 : 1.0\n",
      "                   place = False             neg : pos    =      1.2 : 1.0\n",
      "                      am = False             pos : neg    =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct The Classifier\n",
    "# classifier = nltk.NaiveBayesClassifier.train(tokenizedList)\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = \"This is the best band I've ever heard!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in all_wordsSample}\n",
    "classifier.classify(test_sent_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
